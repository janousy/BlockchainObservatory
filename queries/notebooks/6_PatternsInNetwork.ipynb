{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7595b33",
   "metadata": {},
   "source": [
    "# General Imports and Spark Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef39fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a449f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = pyspark.SparkConf().setAll([\n",
    "    ('spark.executor.memory', '16g'), \n",
    "    ('spark.executor.cores', '4'), \n",
    "    ('spark.cores.max', '4'),\n",
    "    ('spark.driver.memory','16'),\n",
    "    ('spark.executor.instances', '1'),\n",
    "    ('spark.worker.cleanup.enabled', 'true'),\n",
    "    ('spark.worker.cleanup.interval', '60'),\n",
    "    ('spark.worker.cleanup.appDataTtl', '60'),\n",
    "    ('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector:10.0.2')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df9b8b4",
   "metadata": {},
   "source": [
    "Important: In neo4j there is a naming convention, node labels should use camelcase (beginning with uppercase) and relationship labels should use all uppercase with _\n",
    "\n",
    "Notes from Neo4j regarding the Spark Connector\n",
    "We recommend individual property fields to be returned, rather than returning graph entity (node, relationship, and path) types. This best maps to Sparkâ€™s type system and yields the best results. So instead of writing:\n",
    "\n",
    "MATCH (p:Person) RETURN p\n",
    "\n",
    "write the following:\n",
    "\n",
    "MATCH (p:Person) RETURN id(p) AS id, p.name AS name.\n",
    "\n",
    "If your query returns a graph entity, use the labels or relationship modes instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c919598c",
   "metadata": {},
   "source": [
    "# Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a212c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf=config) \\\n",
    "    .appName(\"PatternsInNetwork\") \\\n",
    "    .master(\"spark://172.23.149.212:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27a4bfa",
   "metadata": {},
   "source": [
    "# Pattern Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab3672",
   "metadata": {},
   "source": [
    "## Pattern 1: Large Payments in less than a Day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe93c04",
   "metadata": {},
   "source": [
    "The main goal of this pattern is to detect accounts that have sent a payment transaction which is larger than 100'000 algos (100'000'000'000 Microalgos) and that another account has sent an equal or larger amount further. The results which are returned can be grouped and counted to have an overwiew which addresses made multiple of these calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d0d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for full data query remove the LIMIT 10 phrase\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (a1:Account)-[r1:PAYMENT]->(a2:Account)-[r2:PAYMENT]->(a3:Account)\n",
    "USING INDEX r1:PAYMENT(amount)\n",
    "USING INDEX r2:PAYMENT(amount)\n",
    "WHERE r2.amount > 100000000000 AND r1.amount > 100000000000 AND a1.account <> a2.account AND a2.account <> a3.account\n",
    "WITH DISTINCT r1 AS rela, a1.account AS senderAccount LIMIT 10\n",
    "RETURN senderAccount\n",
    "\"\"\"\n",
    "\n",
    "queryAllData = \"\"\"\n",
    "MATCH (a1:Account)-[r1:PAYMENT]->(a2:Account)-[r2:PAYMENT]->(a3:Account)\n",
    "USING INDEX r1:PAYMENT(amount)\n",
    "USING INDEX r2:PAYMENT(amount)\n",
    "WHERE r2.amount > 100000000000 AND r1.amount > 100000000000 AND a1.account <> a2.account AND a2.account <> a3.account\n",
    "WITH DISTINCT r1 AS rela, a1.account AS senderAccount\n",
    "RETURN senderAccount\n",
    "\"\"\"\n",
    "\n",
    "dfPattern1 = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"query\", queryAllData) \\\n",
    "  .load() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be64a223",
   "metadata": {},
   "source": [
    "Group the data by senderAccount and count how many times the sender has made such a transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern1.show()\n",
    "dfPattern1Grouped = dfPattern1.groupBy(\"senderAccount\").count().sort(col(\"count\").desc())\n",
    "dfPattern1Grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21d70d",
   "metadata": {},
   "source": [
    "Write results to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern1Grouped.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'algorand_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'Patterns_LargePaymentTransactionAccounts_6') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8efb270",
   "metadata": {},
   "source": [
    "## Pattern 2: Accounts that have made many Asset Transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a43a9f",
   "metadata": {},
   "source": [
    "The goal is to find accounts that made many asset transfers between each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for full data query remove the LIMIT 10 phrase\n",
    "\n",
    "# diejenigen die viel kreieren in asset configuration\n",
    "# diejenigen finden welche wie oben das gleiche asset weiter transferiert haben in kurzer zeit, mischung aus 1 und 3\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (a1:Account)-[r:ASSET_TRANSFER]->(a2:Account)\n",
    "USING INDEX r:ASSET_TRANSFER(transferType)\n",
    "WHERE a1.account <> a2.account AND r.transferType=\"transfer\"\n",
    "With a1.account AS senderAccount, count(r) AS rel_count\n",
    "RETURN senderAccount, rel_count\n",
    "ORDER BY rel_count DESC \n",
    "\"\"\"\n",
    "\n",
    "queryAllData = \"\"\"\n",
    "MATCH (a1:Account)-[r:ASSET_TRANSFER]->(a2:Account)\n",
    "USING INDEX r:ASSET_TRANSFER(transferType)\n",
    "WHERE a1.account <> a2.account AND r.transferType=\"transfer\"\n",
    "WITH a1.account AS senderAccount, count(r) AS rel_count\n",
    "RETURN senderAccount, rel_count\n",
    "ORDER BY rel_count DESC \n",
    "\"\"\"\n",
    "\n",
    "dfPattern2 = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"query\", queryAllData) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alternativeQuery = \"\"\"\n",
    "MATCH (a1:Account)-[r:ASSET_TRANSFER]->(a2:Account) \n",
    "WHERE a1.account <> a2.account \n",
    "WITH size((a1:Account)-[r:ASSET_TRANSFER]->(a2:Account) ) AS rel_count, a1.account AS senderAccount, a2.account as rcv LIMIT 10\n",
    "RETURN senderAccount, rcv, rel_count\n",
    "ORDER BY rel_count DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9674ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86745ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern2.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'algorand_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'Patterns_ManyAssetTransferAcc_6') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d79734",
   "metadata": {},
   "source": [
    "## Pattern 3: Accounts that have created a lot of assets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d483c37f",
   "metadata": {},
   "source": [
    "The goal is to find accounts that have sent many asset_configuration transactions to create an asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for full data query remove the LIMIT 10 phrase\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (a1:Account)-[r:ASSET_CONFIGURATION]->(a2:Asset)\n",
    "WHERE r.configurationType = \"creation\"\n",
    "WITH a1.account AS senderAccount, COUNT(r) AS rel_count LIMIT 10\n",
    "RETURN senderAccount, rel_count\n",
    "ORDER BY rel_count DESC\n",
    "\"\"\"\n",
    "\n",
    "queryAllData = \"\"\"\n",
    "MATCH (a1:Account)-[r:ASSET_CONFIGURATION]->(a2:Asset)\n",
    "WHERE r.configurationType = \"creation\"\n",
    "WITH a1.account AS senderAccount, count(r) AS rel_count\n",
    "RETURN senderAccount, rel_count\n",
    "ORDER BY rel_count DESC\n",
    "\"\"\"\n",
    "\n",
    "dfPattern3 = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"query\", queryAllData) \\\n",
    "  .load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b492a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern3.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'algorand_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'Patterns_AccAssetCreation_6') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2090e7a2",
   "metadata": {},
   "source": [
    "## Pattern 4: SmartContract calls where other people have called the same SC shortly after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d624b",
   "metadata": {},
   "source": [
    "The goal of this pattern is to detect smart contracts and people that have all sent transactions to the same SC in a short amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for full data query remove the LIMIT 10 phrase\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (a1:Account)-[r1:APPLICATION_CALL]->(app:Application)<-[r2:APPLICATION_CALL]-(a2:Account)\n",
    "USING INDEX r1:APPLICATION_CALL(blockNumber)\n",
    "USING INDEX r2:APPLICATION_CALL(blockNumber)\n",
    "WHERE a1.account <> a2.account AND r1.blockNumber > 0 AND r2.blockNumber > 0 AND abs(r2.blockNumber - r1.blockNumber) < 17280 \n",
    "WITH a1.account AS account, app.application AS application, r1.blockNumber as blockNumber LIMIT 100 \n",
    "RETURN DISTINCT application, account, blockNumber\n",
    "\"\"\"\n",
    "\n",
    "queryAllData = \"\"\"\n",
    "MATCH (a1:Account)-[r1:APPLICATION_CALL]->(app:Application)<-[r2:APPLICATION_CALL]-(a2:Account)\n",
    "USING INDEX r1:APPLICATION_CALL(blockNumber)\n",
    "USING INDEX r2:APPLICATION_CALL(blockNumber)\n",
    "WHERE a1.account <> a2.account AND r1.blockNumber > 0 AND r2.blockNumber > 0 AND abs(r2.blockNumber - r1.blockNumber) < 17280 \n",
    "WITH a1.account AS account, app.application AS application, r1.blockNumber as blockNumber\n",
    "RETURN DISTINCT application, account, blockNumber\n",
    "\"\"\"\n",
    "\n",
    "dfPattern4 = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"query\", queryAllData) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d93df",
   "metadata": {},
   "source": [
    "Write the results to MongoDB silver collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc9776",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern4.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'algorand_silver') \\\n",
    "  \t.option('spark.mongodb.collection', 'Patterns_ScCallsFromDifferentAccRaw_6') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9502741",
   "metadata": {},
   "source": [
    "Group and count the applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380022b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern4Grouped = dfPattern4.groupBy(\"application\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9983a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern4Grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce67a3eb",
   "metadata": {},
   "source": [
    "Write grouped results to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5747fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern4Grouped.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'algorand_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'Patterns_ScCallsGrouped_6') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d48c92",
   "metadata": {},
   "source": [
    "## Pattern 5: Accounts that had a lot small of transfers between each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695dccb",
   "metadata": {},
   "source": [
    "The goal is to find accounts that made more than 100 transactions between each other with an amount smaller than 0.1 Algos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9403b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for full data query remove the LIMIT 10 phrase\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (a1:Account)-[r:PAYMENT]->(a2:Account) \n",
    "WHERE r.amount < 100000 AND a1.account <> a2.account \n",
    "WITH count(r) AS rel_count, a1.account AS senderAccount, a2.account AS receiverAccount LIMIT 10 \n",
    "WHERE rel_count > 100 \n",
    "RETURN senderAccount, receiverAccount, rel_count\n",
    "ORDER BY rel_count DESC\n",
    "\"\"\"\n",
    "\n",
    "queryAllData = \"\"\"\n",
    "MATCH (a1:Account)-[r:PAYMENT]->(a2:Account) \n",
    "WHERE r.amount < 100000 AND a1.account <> a2.account \n",
    "WITH count(r) AS rel_count, a1.account AS senderAccount, a2.account AS receiverAccount\n",
    "WHERE rel_count > 100 \n",
    "RETURN senderAccount, receiverAccount, rel_count\n",
    "ORDER BY rel_count DESC\n",
    "\"\"\"\n",
    "\n",
    "dfPattern5 = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"query\", queryAllData) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b12f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d021e1",
   "metadata": {},
   "source": [
    "Save the results in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736bf8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern5.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'algorand_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'Patterns_AccountsWithManyPaymentTransactions_6') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce113f0",
   "metadata": {},
   "source": [
    "Sum over all sender accounts to see how many of these transactions were made by each account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c3cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern5Summed = dfPattern5.groupBy(\"senderAccount\").sum(\"rel_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern5Summed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a841d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern5Summed.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'algorand_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'Patterns_AccWithManyPaymentTransactionsSum_6') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae2598",
   "metadata": {},
   "source": [
    "# Patterns that make use of GraphAlgorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc8e13e",
   "metadata": {},
   "source": [
    "Create the graphs projections that are needed for the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd7cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CALL gds.graph.project(\n",
    "  \"paymentGraph\",\n",
    "  \"Account\",                         \n",
    "  {\n",
    "    PAYMENT: {properties: [\"blockNumber\", \"amount\"]}\n",
    "  }           \n",
    ")\n",
    " YIELD\n",
    "  graphName AS graph, nodeProjection, nodeCount AS nodes, relationshipProjection, relationshipCount AS rels\n",
    " RETURN graph, nodeProjection, nodes, relationshipProjection, rels\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "dfPaymentGraphProjection = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"query\", query) \\\n",
    "  .option(\"partitions\", \"1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173862c9",
   "metadata": {},
   "source": [
    "## Pattern 6: Degree Centrality in Payment Subnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849dca9b",
   "metadata": {},
   "source": [
    "The goal of this pattern is to detect degree centralities in asset transfer senders. We search for the 50 nodes with the highest degrees to detect the most important accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CALL gds.degree.stream('paymentGraph')\n",
    "YIELD nodeId, score\n",
    "WITH gds.util.asNode(nodeId).account AS account, score AS degree\n",
    "ORDER BY degree DESC LIMIT 50\n",
    "RETURN account, degree\n",
    "\"\"\"\n",
    "\n",
    "dfPattern6 = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"query\", query) \\\n",
    "  .option(\"partitions\", \"1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b1dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern6.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'algorand_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'Patterns_DegreeCentrality_Top50_6') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9544fd",
   "metadata": {},
   "source": [
    "## Pattern 7: Eigenvector Centrality in Payment Subnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837156e",
   "metadata": {},
   "source": [
    "The goal of this pattern is to detect centralities in asset transfer senders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b8080",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CALL gds.eigenvector.stream('paymentGraph')\n",
    "YIELD nodeId, score\n",
    "WITH gds.util.asNode(nodeId).account AS account, score AS eigenVectorScore\n",
    "ORDER BY eigenVectorScore DESC LIMIT 10\n",
    "RETURN account, eigenVectorScore\n",
    "\"\"\"\n",
    "\n",
    "dfPattern7 = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"query\", query) \\\n",
    "  .option(\"partitions\", \"1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585912a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern7.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'algorand_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'Patterns_EigenvectorCentrality_Top10_6') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba346ae5",
   "metadata": {},
   "source": [
    "# Stopping Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50860854",
   "metadata": {},
   "source": [
    "Stopping context and removing the graph projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb28f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CALL gds.graph.drop('paymentGraph') \n",
    "YIELD graphName \n",
    "RETURN graphName\n",
    "\"\"\"\n",
    "\n",
    "dfPaymentGraphProjection = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"query\", query) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64683194",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af01e02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
