{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c428647",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cef39fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpt\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import Row\n",
    "from graphframes import *\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "\n",
    "from pyspark.sql.functions import col, hex, base64, avg, collect_list, concat, lit, max\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a449f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = pyspark.SparkConf().setAll([\n",
    "    ('spark.executor.memory', '12g'), \n",
    "    ('spark.executor.cores', '3'), \n",
    "    ('spark.cores.max', '3'),\n",
    "    ('spark.driver.memory','1g'),\n",
    "    ('spark.executor.instances', '1'),\n",
    "    ('spark.dynamicAllocation.enabled', 'true'),\n",
    "    ('spark.dynamicAllocation.shuffleTracking.enabled', 'true'),\n",
    "    ('spark.dynamicAllocation.executorIdleTimeout', '60s'),\n",
    "    ('spark.dynamicAllocation.minExecutors', '0'),\n",
    "    ('spark.dynamicAllocation.maxExecutors', '2'),\n",
    "    ('spark.dynamicAllocation.initialExecutors', '1'),\n",
    "    ('spark.dynamicAllocation.executorAllocationRatio', '1')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a212c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/05 14:09:04 WARN Utils: Your hostname, algorand-druid-and-spark resolves to a loopback address: 127.0.0.1; using 172.23.149.212 instead (on interface ens3)\n",
      "22/05/05 14:09:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ff512511-229e-4a83-a296-d619718b3546;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 143ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;1.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ff512511-229e-4a83-a296-d619718b3546\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "22/05/05 14:09:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/05 14:09:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/05 14:09:06 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/05/05 14:09:06 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/05/05 14:09:06 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/05/05 14:09:06 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/05/05 14:09:06 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "22/05/05 14:09:06 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "22/05/05 14:09:06 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "22/05/05 14:09:07 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "# use for 8889\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"Neo4j Testing\") \\\n",
    "    .master(\"spark://172.23.149.212:7077\") \\\n",
    "    .config(conf=config) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136de9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use for 8890\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf=config) \\\n",
    "    .master(\"spark://172.23.149.212:7077\") \\\n",
    "    .appName(\"Neo4j Example\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9a74fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------------------+----+\n",
      "|<id>|<labels>|                name|born|\n",
      "+----+--------+--------------------+----+\n",
      "|  44|[Person]|   Jonathan Lipnicki|1996|\n",
      "| 173|[Person]|Janosch Baltenspe...|1995|\n",
      "| 175|[Person]|      Domenic Fuerer|1998|\n",
      "| 177|[Person]|        Lennart Jung|1998|\n",
      "+----+--------+--------------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/05 14:05:29 WARN SchemaService: Switching to query schema resolution\n"
     ]
    }
   ],
   "source": [
    "# read from neo4j database\n",
    "# how can i change the dabatase to be read from? is it the one that neo4j uses at the moment?\n",
    "# make sure the movie graph is loaded in neo4j, otherwise no data will be shown\n",
    "\n",
    "dfPerson = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\") \\\n",
    "  .option(\"authentication.basic.password\", \"password\") \\\n",
    "  .option(\"labels\", \"Person\") \\\n",
    "  .load()\n",
    "\n",
    "dfPerson.where(\"born > 1990\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35702646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------------------+----+\n",
      "|<id>|<labels>|              name|born|\n",
      "+----+--------+------------------+----+\n",
      "|   1|[Person]|      Keanu Reeves|1964|\n",
      "|   2|[Person]|  Carrie-Anne Moss|1967|\n",
      "|   3|[Person]|Laurence Fishburne|1961|\n",
      "|   4|[Person]|      Hugo Weaving|1960|\n",
      "|   5|[Person]|   Lilly Wachowski|1967|\n",
      "|   6|[Person]|    Lana Wachowski|1965|\n",
      "|   7|[Person]|       Joel Silver|1952|\n",
      "|   8|[Person]|       Emil Eifrem|1978|\n",
      "|  12|[Person]|   Charlize Theron|1975|\n",
      "|  13|[Person]|         Al Pacino|1940|\n",
      "|  14|[Person]|   Taylor Hackford|1944|\n",
      "|  16|[Person]|        Tom Cruise|1962|\n",
      "|  17|[Person]|    Jack Nicholson|1937|\n",
      "|  18|[Person]|        Demi Moore|1962|\n",
      "|  19|[Person]|       Kevin Bacon|1958|\n",
      "|  20|[Person]| Kiefer Sutherland|1966|\n",
      "|  21|[Person]|         Noah Wyle|1971|\n",
      "|  22|[Person]|  Cuba Gooding Jr.|1968|\n",
      "|  23|[Person]|      Kevin Pollak|1957|\n",
      "|  24|[Person]|        J.T. Walsh|1943|\n",
      "+----+--------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPerson.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e089a7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|                name|born|\n",
      "+--------------------+----+\n",
      "|        Lennart Jung|1998|\n",
      "|      Domenic Fuerer|1998|\n",
      "|Janosch Baltenspe...|1995|\n",
      "+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWrite = spark.createDataFrame([\n",
    "    Row(name=\"Lennart Jung\", born=1998),\n",
    "    Row(name=\"Domenic Fuerer\", born=1998),\n",
    "    Row(name=\"Janosch Baltensperger\", born=1995)\n",
    "])\n",
    "dfWrite.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c60934be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/05 14:05:24 WARN SchemaService: Switching to query schema resolution\n"
     ]
    }
   ],
   "source": [
    "dfWrite.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"Overwrite\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"labels\", \":Person\") \\\n",
    "  .option(\"node.keys\", \"name\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1404f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
