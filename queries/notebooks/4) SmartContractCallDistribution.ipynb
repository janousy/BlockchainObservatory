{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5191f81f-e47a-4bf6-be00-4e7277b6f40e",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09df968-e1f0-4e18-a21c-b0f578c21bfd",
   "metadata": {},
   "source": [
    "import, config, creation of spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be10c6ad-b6d2-4e0d-80b9-a77b9e439df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importstatements\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as F     \n",
    "\n",
    "from pyspark.sql import types \n",
    "from pyspark.sql.types import StructField, StringType, LongType, DoubleType, BooleanType, StructType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c0c01b-5925-40c4-959d-052f131311d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config for our sparksession\n",
    "config = pyspark.SparkConf().setAll([\n",
    "    ('spark.executor.memory', '16g'), \n",
    "    ('spark.executor.cores', '4'), \n",
    "    ('spark.cores.max', '8'),\n",
    "    ('spark.driver.memory','2g'),\n",
    "    ('spark.executor.instances', '1'),\n",
    "    ('spark.dynamicAllocation.enabled', 'true'),\n",
    "    ('spark.dynamicAllocation.shuffleTracking.enabled', 'true'),\n",
    "    ('spark.dynamicAllocation.executorIdleTimeout', '60s'),\n",
    "    ('spark.dynamicAllocation.minExecutors', '2'),\n",
    "    ('spark.dynamicAllocation.maxExecutors', '2'),\n",
    "    ('spark.dynamicAllocation.initialExecutors', '1'),\n",
    "    ('spark.dynamicAllocation.executorAllocationRatio', '1'),\n",
    "    ('spark.worker.cleanup.enabled', 'true'),\n",
    "    ('spark.worker.cleanup.interval', '60'),\n",
    "    ('spark.shuffle.service.db.enabled', 'true'),\n",
    "    ('spark.worker.cleanup.appDataTtl', '60'),\n",
    "    ('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector:10.0.2')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b073853f-e6db-40a6-a211-12355b30a931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/08 10:41:30 WARN Utils: Your hostname, algorand-druid-and-spark resolves to a loopback address: 127.0.0.1; using 172.23.149.212 instead (on interface ens3)\n",
      "22/07/08 10:41:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-47c6ccb6-ece9-4071-a06f-0d59ad48835a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector;10.0.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.5.1 in central\n",
      "\t[4.5.1] org.mongodb#mongodb-driver-sync;[4.5.0,4.5.99)\n",
      "\tfound org.mongodb#bson;4.5.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.5.1 in central\n",
      ":: resolution report :: resolve 2854ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.5.1 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector;10.0.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   1   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-47c6ccb6-ece9-4071-a06f-0d59ad48835a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/5ms)\n",
      "22/07/08 10:41:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/07/08 10:41:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/07/08 10:41:36 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/07/08 10:41:36 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/07/08 10:41:36 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/07/08 10:41:36 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/07/08 10:41:36 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "22/07/08 10:41:36 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "22/07/08 10:41:36 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "22/07/08 10:41:36 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "22/07/08 10:41:37 WARN Utils: spark.dynamicAllocation.initialExecutors less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "22/07/08 10:41:37 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "22/07/08 10:41:37 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "#create sparksession\n",
    "#when copying change appName\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf=config) \\\n",
    "    .appName(\"4_smartContractDistibution\") \\\n",
    "    .master(\"spark://172.23.149.212:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f655c24-9690-4812-a813-6471b1954f18",
   "metadata": {},
   "source": [
    "# Loading df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0e75f1-c54c-48ae-9b9f-732c80408c39",
   "metadata": {},
   "source": [
    "creating a schema for tx table, load txtable, load account table, drop from both tables the fields not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1332b4-22bc-4adb-85f2-a647a55abc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a schema so all data quality is ensured\n",
    "schema = StructType([ \\\n",
    "    StructField(\"_id\", StringType(), True), \\\n",
    "    StructField(\"asset\", LongType(), True), \\\n",
    "    StructField(\"extra\", StringType(), True), \\\n",
    "    StructField(\"intra\", LongType(), True), \\\n",
    "    StructField(\"round\", LongType(), True), \\\n",
    "    StructField(\"rr\", LongType(), True), \\\n",
    "    StructField(\"sig\", StringType(), True), \\\n",
    "    StructField(\"txid\", StringType(), True), \\\n",
    "    StructField(\"txn_aamt\", LongType(), True), \\\n",
    "    StructField(\"txn_aclose\", StringType(), True), \\\n",
    "    StructField(\"txn_afrz\", BooleanType(), True), \\\n",
    "    StructField(\"txn_amt\", LongType(), True), \\\n",
    "    StructField(\"txn_apaa\", StringType(), True), \\\n",
    "    StructField(\"txn_apan\", LongType(), True), \\\n",
    "    StructField(\"txn_apap\", StringType(), True), \\\n",
    "    StructField(\"txn_apar\", StringType(), True), \\\n",
    "    StructField(\"txn_apas\", StringType(), True), \\\n",
    "    StructField(\"txn_apat\", StringType(), True), \\\n",
    "    StructField(\"txn_apep\", StringType(), True), \\\n",
    "    StructField(\"txn_apfa\", StringType(), True), \\\n",
    "    StructField(\"txn_apgs\", StringType(), True), \\\n",
    "    StructField(\"txn_apid\", LongType(), True), \\\n",
    "    StructField(\"txn_apls\", StringType(), True), \\\n",
    "    StructField(\"txn_apsu\", StringType(), True), \\\n",
    "    StructField(\"txn_arcv\", StringType(), True), \\\n",
    "    StructField(\"txn_asnd\", StringType(), True), \\\n",
    "    StructField(\"txn_caid\", LongType(), True), \\\n",
    "    StructField(\"txn_close\", StringType(), True), \\\n",
    "    StructField(\"txn_fadd\", StringType(), True), \\\n",
    "    StructField(\"txn_faid\", LongType(), True), \\\n",
    "    StructField(\"txn_fee\", LongType(), True), \\\n",
    "    StructField(\"txn_fv\", LongType(), True), \\\n",
    "    StructField(\"txn_gen\", StringType(), True), \\\n",
    "    StructField(\"txn_gh\", StringType(), True), \\\n",
    "    StructField(\"txn_grp\", StringType(), True), \\\n",
    "    StructField(\"txn_lsig\", StringType(), True), \\\n",
    "    StructField(\"txn_lv\", LongType(), True), \\\n",
    "    StructField(\"txn_lx\", StringType(), True), \\\n",
    "    StructField(\"txn_msig\", StringType(), True), \\\n",
    "    StructField(\"txn_nonpart\", BooleanType(), True), \\\n",
    "    StructField(\"txn_note\", StringType(), True), \\\n",
    "    StructField(\"txn_rcv\", StringType(), True), \\\n",
    "    StructField(\"txn_rekey\", StringType(), True), \\\n",
    "    StructField(\"txn_selkey\", StringType(), True), \\\n",
    "    StructField(\"txn_sig\", StringType(), True), \\\n",
    "    StructField(\"txn_snd\", StringType(), True), \\\n",
    "    StructField(\"txn_type\", StringType(), True), \\\n",
    "    StructField(\"txn_votefst\", LongType(), True), \\\n",
    "    StructField(\"txn_votekd\", LongType(), True), \\\n",
    "    StructField(\"txn_votekey\", StringType(), True), \\\n",
    "    StructField(\"txn_votelst\", LongType(), True), \\\n",
    "    StructField(\"txn_xaid\", LongType(), True), \\\n",
    "    StructField(\"typeenum\", IntegerType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "967c4a6d-661a-46b7-8b8e-2c19b324aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account table to determine which accounts have received rewards\n",
    "dfTx = spark.read.format(\"mongodb\") \\\n",
    "    .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "    .option('spark.mongodb.database', 'algorand') \\\n",
    "    .option('spark.mongodb.collection', 'txn') \\\n",
    "    .option('park.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "    .option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "    .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a1f09ea-18ad-4086-b3e7-7250c4f7b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all unnecessary tables\n",
    "dfTx = dfTx.select(\"round\", \"txn_snd\", \"txn_type\", \"txn_apid\", \"txn_apan\", \"txn_apas\", \"txn_apap\", \"txid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04ebf131-3e59-48d8-9dbe-88b0b4e26470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyreg is either a node which log in to participate in the network or log off\n",
    "dfTx = dfTx.filter(dfTx.txn_type == \"appl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c03442e-096b-4300-b1ca-a2854aa1bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all applications, to count the amount of applications\n",
    "dfApp = spark.read.format(\"mongodb\") \\\n",
    "    .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "    .option('spark.mongodb.database', 'algorand') \\\n",
    "    .option('spark.mongodb.collection', 'app') \\\n",
    "    .option('park.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "    .option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "    .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ea3d96a-ec69-4ed4-ad1e-1afff1eea078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all unnecessary tables\n",
    "dfApp = dfApp.select(\"index\", \"created_at\", \"closed_at\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a392eae-10d3-4e46-a34c-2bef2081251f",
   "metadata": {},
   "source": [
    "# Use Case Distinction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490939a3-bc82-404d-8d4f-b86c7cc5f667",
   "metadata": {},
   "source": [
    "group by use cases and add a column which describes the use case as a word, count the amount of use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad0dd3a6-9de6-4fca-962e-6d8e3b9f4647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use case based grouping\n",
    "#problem two use cases cannot be distinct\n",
    "#null --> create \n",
    "#null --> noOp transaction\n",
    "#1-> opt in\n",
    "#2->close_out\n",
    "#3-> clear state\n",
    "#4-> update sc\n",
    "#5-> delete sc\n",
    "dfTx = dfTx.withColumn(\"usecase\", F.when(F.col('txn_apan')== 1, \"opt_in\")\n",
    "                       .when(F.col('txn_apan')== 2, \"close_out\")\n",
    "                       .when(F.col('txn_apan')== 3, \"clear_state\")\n",
    "                       .when(F.col('txn_apan')== 4, \"updateSC\")\n",
    "                       .when(F.col('txn_apan')== 5, \"deleteSC\")\n",
    "                       .when((F.col('txn_apan').isNull()) & (F.col('txn_apap').isNotNull()), \"createSC\")\n",
    "                       .otherwise(\"NoOp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57344cbb-c817-4e81-a0d7-299f4e45b8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/08 10:42:02 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "applications = dfApp.count()\n",
    "newestRound = dfApp.agg(F.max(\"created_at\")).collect()[0][0]\n",
    "\n",
    "#write amount of applications in gold table\n",
    "result = spark.createDataFrame(\n",
    "    [\n",
    "        (applications, newestRound)  # create your data here, be consistent in the types.\n",
    "        \n",
    "    ],\n",
    "    [\"AmountOfApplications\", \"CreationRound\"]  # add your column names here\n",
    ")\n",
    "\n",
    "result.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "  \t.mode(\"append\") \\\n",
    "    .option('spark.mongodb.database', 'algorand_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'ApplicationCount_4') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6392b7c0-4d8a-4896-9f60-7266f05a5661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|  usecase|count|\n",
      "+---------+-----+\n",
      "| createSC|  372|\n",
      "|     NoOp| 2876|\n",
      "| updateSC|  195|\n",
      "|   opt_in|  266|\n",
      "| deleteSC|   74|\n",
      "|close_out|  178|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTxGroupUseCases = dfTx.groupBy(\"usecase\").count()\n",
    "#clear_state, close_out, createSC, deleteSC, NoOp, opt_in, updateSC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d516f51e-87f0-43a5-bf79-1bf68af6340b",
   "metadata": {},
   "source": [
    "store all application transactions on the gold table grouped by its use case and counted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2173f681-ac52-4588-91cc-be6c4481bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add creation round to every group, so it can be distinguished when the group was saved\n",
    "\n",
    "dfTxGroupUseCasesGold = dfTxGroupUseCases.withColumn(\"CreationRound\", F.lit(newestRound))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520a5be-36b8-4cc7-9691-0aa83c41273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/08 11:34:02 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n"
     ]
    }
   ],
   "source": [
    "dfTxGroupUseCasesGold.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "  \t.mode(\"append\") \\\n",
    "    .option('spark.mongodb.database', 'algorand_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'ApplicationTransactionsByUseCase_4') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c83bf5-5bdc-417c-a125-95977a07e86d",
   "metadata": {},
   "source": [
    "# Preparation for Use Case Counter Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a525625a-1578-4a1c-9293-91c3047fa0ff",
   "metadata": {},
   "source": [
    "create original python object with collect, create from tuples lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156990fa-968f-46ca-ba28-cc0a3aadf985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect so a python object is created\n",
    "graph = dfTxGroupUseCases.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36e5a5-4ae6-491c-b6e3-ce7be3c71284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph, histogram x-axis unix time when starting\n",
    "#graph = dfTxGroupUseCases.select(\"count\")\n",
    "#graph = dfTxGroupUseCases.collect()\n",
    "\n",
    "#convert row[\"data\"] to only data\n",
    "UCnames = [row[0] for (row) in graph]\n",
    "UCvalues = [row[1] for (row) in graph]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3425c173-d7f5-40da-9073-39253bba141d",
   "metadata": {},
   "source": [
    "# Use Case Counter Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42700547-3c1d-43f5-816e-07a3372fbd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(len(UCnames)):\n",
    "    plt.bar(UCnames[i], UCvalues[i], width = 0.4)\n",
    "    \n",
    "plt.title(\"Smart Contract Use Case Transactions\", loc ='center', pad = None)\n",
    "plt.savefig('/home/ubuntu/apps/figures/4_ScDistribution/SC_Use_Cases.jpg', dpi= 200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6ce70-1b65-4c19-95a1-d4c6e7253c50",
   "metadata": {},
   "source": [
    "# Preparation and Graph for Smart Contract Calls over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b35427c-01e3-4634-8d45-97685b141ece",
   "metadata": {},
   "source": [
    "first preparation, than plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d4b29-a005-45c1-81e7-7fec292c4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diagramm: how much sc calls were made\n",
    "\n",
    "graph = dfTx.select(\"round\")\n",
    "\n",
    "#preparation for graph\n",
    "graph = graph.collect()\n",
    "\n",
    "#convert row[\"data\"] to only data\n",
    "SCcalls = [row[0] for (row) in graph]\n",
    "\n",
    "#calculate the mean of all miner rewards\n",
    "mean_round = dfTx.agg(F.mean(\"round\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ebf008-9e3b-405d-a855-40c90015987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#min\n",
    "minSCround = dfTx.agg(F.min(\"round\")).collect()[0][0]\n",
    "maxSCround = dfTx.agg(F.max(\"round\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c3977-eb6a-4953-af74-ed2d55aaeffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram x-axis round when starting participating\n",
    "#how many bars in the histogram should be plotted\n",
    "\n",
    "bin_size = 50\n",
    "#distribute bins log(equally) over the whole data\n",
    "mybins = np.logspace(np.log10(minSCround), np.log10(maxSCround), bin_size )\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(SCcalls, bins = mybins)\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Blockround\")\n",
    "plt.ylabel(\"Number of Smart Contract Calls\")\n",
    "plt.title(\"Smart Contract Call Distribution (Blockround)\", loc ='center', pad = None)\n",
    "plt.savefig('/home/ubuntu/apps/figures/4_ScDistribution/SC_Call_Distribution_blockround.jpg', dpi= 200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc56090-8c75-4e34-ab45-3d375e043389",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef83e337-5005-4f2a-a293-25db4b0ff41d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
