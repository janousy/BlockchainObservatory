{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c428647",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7595b33",
   "metadata": {},
   "source": [
    "# General Imports and Spark Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3cef39fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53a449f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = pyspark.SparkConf().setAll([\n",
    "    ('spark.executor.memory', '16g'), \n",
    "    ('spark.executor.cores', '4'), \n",
    "    ('spark.cores.max', '4'),\n",
    "    ('spark.driver.memory','64g'),\n",
    "    ('spark.executor.instances', '1'),\n",
    "    ('spark.worker.cleanup.enabled', 'true'),\n",
    "    ('spark.worker.cleanup.interval', '60'),\n",
    "    ('spark.worker.cleanup.appDataTtl', '60'),\n",
    "    ('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector:10.0.2')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df9b8b4",
   "metadata": {},
   "source": [
    "Important: In neo4j there is a naming convention, node labels should use camelcase (beginning with uppercase) and relationship labels should use all uppercase with _\n",
    "\n",
    "Notes from Neo4j regarding the Spark Connector\n",
    "We recommend individual property fields to be returned, rather than returning graph entity (node, relationship, and path) types. This best maps to Sparkâ€™s type system and yields the best results. So instead of writing:\n",
    "\n",
    "MATCH (p:Person) RETURN p\n",
    "\n",
    "write the following:\n",
    "\n",
    "MATCH (p:Person) RETURN id(p) AS id, p.name AS name.\n",
    "\n",
    "If your query returns a graph entity, use the labels or relationship modes instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c919598c",
   "metadata": {},
   "source": [
    "# Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a212c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182337 [Thread-4] WARN  org.apache.spark.util.Utils  - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "189331 [Thread-4] WARN  org.apache.spark.ExecutorAllocationManager  - Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf=config) \\\n",
    "    .appName(\"PatternsInNetwork\") \\\n",
    "    .master(\"spark://172.23.149.212:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27a4bfa",
   "metadata": {},
   "source": [
    "# Pattern Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab3672",
   "metadata": {},
   "source": [
    "## Pattern 1: Large Payments in less than a Day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe93c04",
   "metadata": {},
   "source": [
    "The main goal of this pattern is to detect accounts that have sent a payment transaction which is larger than 100'000 algos and that another account has sent an equal or larger amount further in less than a day. A day can be represented by the blocknumber difference. As a block takes usually 5 seconds to create, there are 17280 blocks created daily. The results which are returned can be grouped and counted to have an overwiew which addresses made multiple of these calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d1d0d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for full data query remove the LIMIT 10 phrase\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (a1:Account)-[r1:PAYMENT]->(a2:Account)-[r2:PAYMENT]->(a3:Account) \n",
    "WHERE a1.account <> a2.account AND r1.amount > 100000000 and r2.amount > 100000000 and r1.blockNumber > 0 and r2.blockNumber > 0 and 0 < r2.blockNumber - r1.blockNumber < 17280 \n",
    "WITH a1.account AS senderAccount LIMIT 10 \n",
    "RETURN senderAccount\n",
    "\"\"\"\n",
    "\n",
    "dfPattern1 = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"query\", query) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21d70d",
   "metadata": {},
   "source": [
    "Write results to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3ba241b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       senderAccount|\n",
      "+--------------------+\n",
      "|hcm0TFElFZuDv652p...|\n",
      "|peoH04okZMbRCx1zR...|\n",
      "|HrMr/5WVzj3n690c4...|\n",
      "|aiJ6oRQbnowaYL4HB...|\n",
      "|vlbsPa3gU3e6aQVEB...|\n",
      "|8inyrMMFCQLZVsCmF...|\n",
      "|1AN5tmjXGWsGACxSY...|\n",
      "|/0QK7CnRG355cTp/z...|\n",
      "|aiJ6oRQbnowaYL4HB...|\n",
      "|hgZAE/fY2rV909K1j...|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+-----+\n",
      "|       senderAccount|count|\n",
      "+--------------------+-----+\n",
      "|aiJ6oRQbnowaYL4HB...|    2|\n",
      "|hcm0TFElFZuDv652p...|    1|\n",
      "|HrMr/5WVzj3n690c4...|    1|\n",
      "|peoH04okZMbRCx1zR...|    1|\n",
      "|/0QK7CnRG355cTp/z...|    1|\n",
      "|vlbsPa3gU3e6aQVEB...|    1|\n",
      "|8inyrMMFCQLZVsCmF...|    1|\n",
      "|1AN5tmjXGWsGACxSY...|    1|\n",
      "|hgZAE/fY2rV909K1j...|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPattern1.show()\n",
    "dfPattern1 = dfPattern1.groupBy(\"senderAccount\").count().sort(col(\"count\").desc())\n",
    "dfPattern1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe59ccc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174937188 [Thread-4] WARN  org.apache.spark.sql.util.CaseInsensitiveStringMap  - Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfPattern1.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'algorand_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'Patterns_LargePaymentTransactionAccounts_6') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8efb270",
   "metadata": {},
   "source": [
    "## Pattern 2: Accounts that have created NFTs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a43a9f",
   "metadata": {},
   "source": [
    "The goal of this query is to detect accounts that have created more than 10 NFT's in one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match (a1:Account)-[r1:PAYMENT]->(a2:Account)-[r2:PAYMENT]->(a3:Account) where a1.account <> a2.account and r1.amount > 100000000 and r2.amount > 100000000 and r2.blockNumber - r1.blockNumber < 17280 return a1.account as senderAccount limit 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2090e7a2",
   "metadata": {},
   "source": [
    "## Pattern 3: SmartContract calls where other people have called the same SC shortly after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d624b",
   "metadata": {},
   "source": [
    "The goal of this pattern is to detect smart contracts and people that have all sent transactions to the same SC in a short amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for full data query remove the LIMIT 10 phrase\n",
    "\n",
    "query1 = \"\"\"\n",
    "MATCH (a1:Account)-[r1:APPLICATION_CALL]->(app:Application)<-[r2:APPLICATION_CALL]-(a2:Account) \n",
    "WHERE a1.account <> a2.account AND r1.blockNumber > 0 AND r2.blockNumber > 0 AND abs(r2.blockNumber - r1.blockNumber) < 17280 \n",
    "WITH a1.account AS account, app.application AS application LIMIT 10 \n",
    "RETURN DISTINCT application, account\n",
    "\"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (a1:Account)-[r1:APPLICATION_CALL]->(app:Application)<-[r2:APPLICATION_CALL]-(a2:Account) \n",
    "WHERE a1.account <> a2.account AND r1.blockNumber > 0 AND r2.blockNumber > 0 AND abs(r2.blockNumber - r1.blockNumber) < 17280 \n",
    "WITH a1.account AS account, app.application AS application\n",
    "RETURN DISTINCT application, account\n",
    "\"\"\"\n",
    "\n",
    "dfPattern3 = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"query\", query) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPattern3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d93df",
   "metadata": {},
   "source": [
    "Write the results into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21bc9776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83778629 [Thread-4] WARN  org.apache.spark.sql.util.CaseInsensitiveStringMap  - Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfPattern3.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'algorand_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'Patterns_ScCallsFromDifferentAcc_6') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d48c92",
   "metadata": {},
   "source": [
    "## Pattern 4: Accounts that had a lot of transfers between each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695dccb",
   "metadata": {},
   "source": [
    "The goal is to find accounts that made more than 100 transactions between each other with an amount smaller than 100 Algos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9403b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for full data query remove the LIMIT 10 phrase\n",
    "\n",
    "query1 = \"\"\"\n",
    "MATCH (a1:Account)-[r:PAYMENT]->(a2:Account) \n",
    "WHERE r.amount < 100000 AND a1.account <> a2.account \n",
    "WITH count(r) AS rel_count, a1.account AS senderAccount, a2.account AS receiverAccount LIMIT 10 \n",
    "WHERE rel_count > 100 \n",
    "RETURN senderAccount, receiverAccount, rel_count\n",
    "ORDER BY rel_count DESC\n",
    "\"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (a1:Account)-[r:PAYMENT]->(a2:Account) \n",
    "WHERE r.amount < 100000 AND a1.account <> a2.account \n",
    "WITH count(r) AS rel_count, a1.account AS senderAccount, a2.account AS receiverAccount\n",
    "WHERE rel_count > 100 \n",
    "RETURN senderAccount, receiverAccount, rel_count\n",
    "ORDER BY rel_count DESC\n",
    "\"\"\"\n",
    "\n",
    "dfPattern4 = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"query\", query) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15b12f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 32:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|       senderAccount|     receiverAccount|rel_count|\n",
      "+--------------------+--------------------+---------+\n",
      "|7OUGoX3hg950O7LF5...|oQY98gUjLRNgQVqyg...|    19306|\n",
      "|7OUGoX3hg950O7LF5...|iRhe80TuSql+RpNPP...|    19306|\n",
      "|7OUGoX3hg950O7LF5...|xQbYMccFxcDsIGRYy...|    19306|\n",
      "|7OUGoX3hg950O7LF5...|1YbO3KX7m2KBrydeF...|    19305|\n",
      "|7OUGoX3hg950O7LF5...|aCYVTqsazIyWyYgpo...|    16755|\n",
      "|7OUGoX3hg950O7LF5...|hTapIPF+i+W+MImEO...|    16755|\n",
      "|7OUGoX3hg950O7LF5...|x8IMpA860wzXTCVAG...|    16754|\n",
      "|7OUGoX3hg950O7LF5...|jA0BZe0eV6eA+fid3...|    16754|\n",
      "|7OUGoX3hg950O7LF5...|+oR7zi4ucUJZCVWHc...|    16754|\n",
      "|7OUGoX3hg950O7LF5...|xjQhuSoCIUsugjmaV...|    16753|\n",
      "+--------------------+--------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfPattern4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d021e1",
   "metadata": {},
   "source": [
    "Save the results in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "736bf8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175044676 [Thread-4] WARN  org.apache.spark.sql.util.CaseInsensitiveStringMap  - Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfPattern4.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'algorand_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'Patterns_AccountsWithManyPaymentTransactions_6') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae2598",
   "metadata": {},
   "source": [
    "# Patterns that make use of GraphAlgorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc8e13e",
   "metadata": {},
   "source": [
    "Create the graphs projections that are needed for the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ddec753",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    " CALL gds.graph.project(\n",
    "  \"paymentGraph\",\n",
    "  \"Account\",                         \n",
    "  {\n",
    "    PAYMENT: {properties: [\"blockNumber\", \"amount\"]}\n",
    "  }           \n",
    ")\n",
    " YIELD\n",
    "  graphName AS graph, nodeProjection, nodeCount AS nodes, relationshipProjection, relationshipCount AS rels\n",
    " RETURN graph, nodeProjection, nodes, relationshipProjection, rels\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "dfPaymentGraphProjection = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"query\", query) \\\n",
    "  .option(\"partitions\", \"1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173862c9",
   "metadata": {},
   "source": [
    "## Pattern 5: Degree Centrality in Payment Senders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849dca9b",
   "metadata": {},
   "source": [
    "The goal of this pattern is to detect degree centralities in asset transfer senders. We search for the 50 nodes with the highest degrees to detect the most important accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c235eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CALL gds.degree.stream('paymentGraph')\n",
    "YIELD nodeId, score\n",
    "WITH gds.util.asNode(nodeId).account AS account, score AS degree\n",
    "ORDER BY degree DESC limit 50\n",
    "RETURN account, degree\n",
    "\"\"\"\n",
    "\n",
    "dfPattern5 = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"query\", query) \\\n",
    "  .option(\"partitions\", \"1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "092b1dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 34:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             account|   degree|\n",
      "+--------------------+---------+\n",
      "|fFAiP2bRer9k3bTSg...|6504460.0|\n",
      "|k3ZZQqtbbSRtH4CZA...|5071150.0|\n",
      "|9orEqXoI5YtIu44xk...|3515002.0|\n",
      "|7OUGoX3hg950O7LF5...|1642024.0|\n",
      "|6ryd57z3fIj/GPds1...|1602807.0|\n",
      "|k//OpTHinxgS6cnqg...| 644163.0|\n",
      "|UGfOsbi2Q/pWY9x3v...| 576457.0|\n",
      "|fFAuGRCaSj1LKCQMD...| 479382.0|\n",
      "|PSeTViEF0j5/99D/o...| 473470.0|\n",
      "|peoH04okZMbRCx1zR...| 328767.0|\n",
      "|Z7nV8rzyDgyjW0uMK...| 285710.0|\n",
      "|Xalg5fUMtcmMI8/0u...| 273877.0|\n",
      "|M9xnoAs0RQqazEtnV...| 264448.0|\n",
      "|iNQ6/FV11ndp1wpFJ...| 227239.0|\n",
      "|bbz5NtJ77OUOH7GBO...| 196938.0|\n",
      "|Q0jCBw2XUaEy/En2H...| 149829.0|\n",
      "|FFc9/RB3H94ir+XH+...| 108081.0|\n",
      "|1SC4OajDfSHywS88T...|  96464.0|\n",
      "|3uZHp1X66koXtE8vl...|  96277.0|\n",
      "|wS+i/DTz5ouQcFB5L...|  92565.0|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfPattern5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "870c4e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174331967 [Thread-4] WARN  org.apache.spark.sql.util.CaseInsensitiveStringMap  - Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfPattern5.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'algorand_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'Patterns_DegreeCentrality_Top50') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9544fd",
   "metadata": {},
   "source": [
    "## Pattern 6: Eigenvector Centrality in Payment Senders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837156e",
   "metadata": {},
   "source": [
    "The goal of this pattern is to detect centralities in asset transfer senders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd2b8080",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CALL gds.eigenvector.stream('paymentGraph')\n",
    "YIELD nodeId, score\n",
    "WITH gds.util.asNode(nodeId).account AS account, score as eigenVectorScore\n",
    "ORDER BY eigenVectorScore DESC limit 10\n",
    "RETURN account, eigenVectorScore\n",
    "\"\"\"\n",
    "\n",
    "dfPattern6 = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"query\", query) \\\n",
    "  .option(\"partitions\", \"1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "387b9216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             account|    eigenVectorScore|\n",
      "+--------------------+--------------------+\n",
      "|fFAiP2bRer9k3bTSg...|  0.9999999999999882|\n",
      "|t4wIVEExOPn4WuJ80...|1.537406877245335E-7|\n",
      "|fFAuGRCaSj1LKCQMD...|8.762774427687245...|\n",
      "|PSeTViEF0j5/99D/o...|7.935811578806059...|\n",
      "|Y/Nb1zwuKQ1WeXjPw...|4.727240903047691...|\n",
      "|+/BT4moSzMB+a6+vC...|4.72723981289393E-14|\n",
      "|DxXEziPamwtB3jgY7...|2.363619906446965...|\n",
      "|gYR87zBKZh28v3Nze...|2.363619906446965...|\n",
      "|1SC4OajDfSHywS88T...|2.422718774106649...|\n",
      "|E6qBx6zBP9/dgxFcK...|4.918362989740098...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfPattern6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "585912a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174307838 [Thread-4] WARN  org.apache.spark.sql.util.CaseInsensitiveStringMap  - Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfPattern6.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.212:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'algorand_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'Patterns_EigenvectorCentrality_Top10') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba346ae5",
   "metadata": {},
   "source": [
    "# Stopping Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1a3d6",
   "metadata": {},
   "source": [
    "Stopping context and removing the graph projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0800f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CALL gds.graph.drop('paymentGraph') \n",
    "YIELD graphName \n",
    "RETURN graphName\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "dfPaymentGraphProjection = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.212:7687\") \\\n",
    "  .option(\"query\", query) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64683194",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
