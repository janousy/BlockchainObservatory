1) The ports below were opened:
  "35000 required to be externally visible" on the network
  "7777 RPC endpoint for interaction with casper-client"
  "8888 REST endpoint for status and metrics (having this accessible allows your node to be part of network status)"
  "9999 "SSE endpoint for event stream"
  
2) We also enabled firewall on the VM and opened the ports with the commands below:

  #Enable firewall
  sudo ufw enable

  #Add the ports
  sudo ufw allow 35000/tcp
  sudo ufw allow 7777/tcp
  sudo ufw allow 8888/tcp
  sudo ufw allow 9999/tcp

  #Check status of allowed ports
  sudo ufw status verbose

3) One can check if ports are open also from the network by running the command 
    curl -s http://13.91.109.211:8888/status | jq -e '.peers[] | select(.address == "<your ip>:35000")'

  or by checking on CSPR Live website (https://cspr.live/tools/peers). In the command <your ip> part should be replaced by your node ip 
  which can be found by running the command below. For our node, node ip was found as 130.60.24.13
    curl https://ipinfo.io/ip

4) We should create a user because the root user and casper user shouldn’t be used to run the set up commands 
due to file permission problems with a root user. The commands below can be run to create the user in sudo group 
and check which user groups it belongs to.

  #Create the user
  sudo adduser username

  #Add user to the sudo group
  sudo adduser username sudo

  #Check which groups the user belongs to:
  id username
  
5) We updated the package repositories, and installed the prerequisites and helpers by running the commands below:

  #Update package repositories
  sudo apt-get update

  #Install prerequisites
  sudo apt install -y dnsutils software-properties-common git

  #Install helpers
  sudo apt install jq -y

6) If one has tried installing casper node before on the VM, they should run the commands below to remove the previous version. 
Also, one should remove any casper node setup related files under the associated disk, in our case under /mnt.

  sudo systemctl stop casper-node-launcher.service
  sudo apt remove -y casper-client
  sudo apt remove -y casper-node
  sudo apt remove -y casper-node-launcher
  sudo rm /etc/casper/casper-node-launcher-state.toml
  sudo rm -rf /etc/casper/1_*
  sudo rm -rf /etc/casper/validator_keys
  sudo rm -rf /var/lib/casper/*
  
7) Next, we ran the commands below to add the Casper repository and to install the Casper node software. 

  #Add the Casper repository under /etc/apt 
  echo "deb [arch=amd64] https://repo.casperlabs.io/releases" bionic main | sudo tee -a /etc/apt/sources.list.d/casper.list

  #Add the public gpg key
  curl -O https://repo.casperlabs.io/casper-repo-pubkey.asc
  sudo apt-key add casper-repo-pubkey.asc
  #casper-repo-pubkey.asc is added under /mnt/casper.

  #Update the package manager
  sudo apt update

  #Verify if the package is added
  sudo apt list casper-client

  #Install the casper-node-launcher
  sudo apt install casper-node-launcher -y
  #This command also generates a new user and a group called "casper". The configuration file, casper-node-launcher.service, is placed under /lib/systemd/system. After the set up of the node finished, Casper node is started with the application located here: /usr/bin/casper-node-launcher. Moreover, log and error log files are located in these directories, respectively: /var/log/casper/casper-node.log, /var/log/casper/casper-node.stderr.log.

  #Install the casper-client
  sudo apt install casper-client -y
  
8) Then, we installed the protocols, created validator keys and obtained a trusted hash by running the commands below.

#Install protocols
sudo -u casper /etc/casper/node_util.py stage_protocols casper.conf

#Create validator keys
sudo -u casper casper-client keygen /etc/casper/validator_keys

#Obtain a trusted hash
sudo sed -i "/trusted_hash =/c\trusted_hash = '$(casper-client get-block --node-address http://3.14.161.135:7777 -b 20 | jq -r .result.block.hash | tr -d '\n')'" /etc/casper/1_0_0/config.toml

9) At this point the node is ready to be started, but we would like to use the bigger disk mounted under /mnt 
and which is more than 2TB as the mainnet data directory. By default the data directory is under /var/lib/casper/casper-node. 
So, first we created a new directory called "casper-mainnet" under /mnt. The directory name can be chosen freely. 
Then, to change the data directory, we replace all occurences of /var/lib/casper/casper-node with /mnt/casper-mainnet in 
config.toml and config-example.toml files under /etc/casper/1_x_x.There are 12 directories under /etc/casper representing different versions: 
1_0_0, 1_1_0, 1_1_2, 1_2_0, 1_2_1, 1_3_2, 1_3_4, 1_4_1, 1_4_3, 1_4_4, 1_4_5, 1_4_6. 

10) Besides, we also changed the value of the "DB_PATH" parameter of "class NodeUtil" in "node_util.py" file under /etc/casper directory:
DB_PATH = Path("/var/lib/casper/casper-node") is modified into DB\_PATH = Path("/mnt/casper-mainnet")

11) Finally, we changed the ownership of the /mnt/casper-mainnet directory to the "casper" user as below, 
so that the node that would use the new data directory could be started successfully.

  sudo chown -R casper:casper /mnt/casper-mainnet
  
12) Lastly, commands below were used to start the node and watch its syncing status. 

#Start the node either using the commands below:
sudo /etc/casper/node_util.py rotate_logs
sudo /etc/casper/node_util.py start

#Or the commands below:
sudo systemctl enable casper-node-launcher
sudo systemctl start casper-node-launcher

#Monitor the status of the data loading
/etc/casper/node_util.py watch

#An example output of a node which is syncing data successfully can be seen 
#in the Casper GitHub(https://github.com/casper-network/casper-node/wiki/Mainnet-Node-Installation-Instructions#monitor-the-node-syncing). 
#The "RPC" status must be seen as "Ready" and there must be a value for the "Latest Block". 

#One can also use the command below to monitor the syncing status:
watch -n 5 ‘echo -n "Peer Count: "; curl -s localhost:8888/status | jq “.peers | length”; echo; echo “last_added_block_info:”; curl -s localhost:8888/status | jq .last_added_block_info; echo; echo “casper-node-launcher status:”; systemctl status casper-node-launcher’

#Or the command below to monitor the node's running status:
sudo systemctl status casper-node-

#Check the last added block information:
curl -s localhost:8888/status | jq .last_added_block_info

#Stop the node using the commands below:
sudo systemctl stop casper-node-launcher
sudo systemctl disable casper-node-launcher

13) You can check if the node has finished its initial syncing by comparing the value corresponding to last_added_block_info 
to the "latest block height" on CSPR Live website(https://cspr.live/blocks). According to what Casper Support informed, 
it lasts several hours for the node to initialize the database, to form peer connections and to ask for blocks. 
They predict that whole mainnet data can be synced in about a week.

When the node was started for the first time, data files were created automatically by casper-node-launcher 
under the data directory /mnt/casper-mainnet and its (data.lmdb) size was increasing in time.






